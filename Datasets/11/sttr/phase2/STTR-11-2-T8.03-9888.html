<HTML>
<HEAD>
	<TITLE>Adaptive bio-inspired navigation for planetary exploration</TITLE>
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<TABLE width="100%" border="0" align="center">
  <TR>
	<TD>
		<H4 align=right>NASA STTR 2011 Solicitation</H4>
		<CENTER><H2>FORM B - PROPOSAL SUMMARY</H2></CENTER>
		<HR align="left" width="100%" SIZE="3" noShade>

		<TABLE cellSpacing="5" cellPadding="0" border="0">
			<TBODY>
				<TR>
					<TD><B>PROPOSAL NUMBER:</B></TD>
					<TD>11-2&nbsp;<B>T8.03-9888</B></TD>
				</TR>
				<TR>
					<TD><B>PHASE 1 CONTRACT NUMBER:</B></TD>
					<TD>NNX12CG32P</TD>
				</TR>
				<TR>
					<TD><B>RESEARCH SUBTOPIC TITLE:</B></TD>
					<TD>Autonomous Navigation in GNSS-Denied Environments</TD>
				</TR>
				<TR>
					<TD><B>PROPOSAL TITLE:</B></TD>
					<TD>Adaptive bio-inspired navigation for planetary exploration</TD>
				</TR>
			</TBODY>
		</TABLE>
		<P>

			<TABLE CELLSPACING="5" CELLPADDING="0" BORDER="0">
			<TR>
				<TD COLSPAN="2" ALIGN="LEFT">
					<U>SMALL&nbsp;BUSINESS&nbsp;CONCERN&nbsp;(SBC):</U>
				</TD>
				<TD COLSPAN="2" ALIGN="LEFT">
					<U>RESEARCH&nbsp;INSTITUTION&nbsp;(RI):</U>
				</TD>
			</TR>
			<TR>
				<TD>NAME:</TD>
				<TH ALIGN="LEFT">Neurala LLC</TH>
				<TD>NAME:</TD>
				<TH ALIGN="LEFT">Trustees of Boston University</TH>
			</TR>
			<TR>
				<TD>STREET:</TD>
				<TH ALIGN="LEFT">846 East 3rd St</TH>
				<TD>STREET:</TD>
				<TH ALIGN="LEFT">881 Commonwealth Avenue</TH>
			</TR>
			<TR>
				<TD>CITY:</TD>
				<TH ALIGN="LEFT">Boston</TH>
				<TD>CITY:</TD>
				<TH ALIGN="LEFT">Boston</TH>
			</TR>
			<TR>
				<TD>STATE/ZIP:</TD>
				<TH ALIGN="LEFT">MA &nbsp;02217 - 2359</TH>
				<TD>STATE/ZIP:</TD>
				<TH ALIGN="LEFT">MA &nbsp;02215 - 1300</TH>
			</TR>
			<TR>
				<TD>PHONE:</TD>
				<TH ALIGN="LEFT">(617) 256-0026</TH>
				<TD>PHONE:</TD>
				<TH ALIGN="LEFT">(617) 353-4365</TH>
			</TR>
			</TABLE>
		<P>
		<B>PRINCIPAL INVESTIGATOR/PROJECT MANAGER</B> <FONT size=-1>(Name, E-mail, Mail Address, City/State/Zip, Phone)</FONT><BR>
		Anatoly Gorshechnikov<BR>
		anatoli@bu.edu<BR>
		677 Beacon St<BR>
		Boston, MA 02215 - 1300<BR>
		(617) 353-8771<BR>

		<P>
		<B>Estimated Technology Readiness Level (TRL) at beginning and end of contract: </B>
		 <br>Begin: 4
		 <br>End: 5

		<P>
		<B>TECHNICAL ABSTRACT</B> (Limit 2000 characters, approximately 200 words)
		<BR>
		Exploration of planetary environments with current robotic technologies relies on human control and power-hungry active sensors to perform even the most elementary low-level functions. Ideally, a robot would be able to autonomously explore novel environments, memorize locations of obstacles or objects, learn about objects, build and update an environment map, and return to a safe location. All of these tasks constitute typical activities efficiently performed by animals on a daily basis. The primary objective of the proposed research is to develop a biologically-inspired neuromorphic application that will translate the above-mentioned functionalities into an autonomous robot or unmanned aerial system (UAS). The Phase I effort implemented a neuromorphic system capable of exploring an unknown environment, avoiding obstacles, and returning to base for refuel/recharge without the use of a Global Navigation Satellite System (GNSS). This system was successfully tested in a Mars-like virtual environment and a simple robot. Leveraging Phase I results, the Phase II effort will develop visual processing based on passive sensors in order to find, identify, localize and interact with objects and use this information to enhance navigation capabilities. Neurala's neuromorphic application will also allow for human guidance through an intuitive user interface. Low-power hardware will be evaluated to facilitate real-time performance in robots and unmanned platforms.
		<P>
		<B>POTENTIAL NASA COMMERCIAL APPLICATIONS</B> (Limit 1500 characters, approximately 150 words)
		<BR>
		The technology will have a transformative impact on space exploration, and is directly relevant in addressing the key attributes of autonomy to support NASA missions, stated in the OCT roadmap for Robotics, Tele-Robotics and Autonomous Systems (TA04) as, "the ability for complex decision making, including autonomous mission execution and planning, the ability to self-adapt as the environment in which the system is operating changes, and the ability to understand system state and react accordingly." This work also addresses two space technology grand challenges which aim to enable transformational space exploration and scientific discovery: all access mobility and surviving extreme space environments as well as one grand challenge, telepresence in space, aimed at expanding human presence in space. NASA will be able to use this technology for autonomous exploration and mapping as well as in hostile environments in which telepresence and autonomous control will be employed.
		<P>
		<B>POTENTIAL NON-NASA COMMERCIAL APPLICATIONS</B> (Limit 1500 characters, approximately 150 words)
		<BR>
		Neurala's neuromorphic application has wide-ranging utility in robotics. It makes use of passive sensors, does not require GNSS for navigation, and incorporates training without explicit programming which taken together will reduce development costs and time while simultaneously increasing the robustness of existing robotic systems. The proposed Phase II innovation brings relevance and added benefit to the following market sectors:<BR>Defense &#150; Unmanned Aerial Systems (UAS), surveillance, patrol, rescue, demining;<BR>Business &#150; telepresence;<BR>Home &#150; cleaning;<BR>Healthcare &#150; remote diagnosis, assistive living; and<BR>Agriculture &#150; autonomous seeding, crop assessment, wildlife conservation.<BR>Neurala will initially focus on a new and emerging teleoperated robots (or telepresence) market as well as the more mature and established UAS sectors. Neurala's technology enables telepresence robots, such as iRobot's RP-VITA, to learn an internal map of rooms, obstacles, and objects of interest. Neurala's solution will also provide collision- and GNSS-free navigation and control-less travel for UAS systems.
		<P>

		<TABLE cellSpacing="1" cellPadding="2" width="100%" border="0">
		  <TBODY>
		  <TR>
		      <TD><DIV align="left"><B>TECHNOLOGY TAXONOMY MAPPING</B> (NASA's technology taxonomy has been developed by the SBIR-STTR program to disseminate awareness of proposed and awarded R/R&D in the agency. It is a listing of over 100 technologies, sorted into broad categories, of interest to NASA.)</DIV>


		</TD>
	      </TR>
		  <TR>
		    <TD>
		      <DIV align="left">
			      Air Transportation & Safety<BR>
			      Algorithms/Control Software & Systems (see also Autonomous Systems)<BR>
			      Autonomous Control (see also Control & Monitoring)<BR>
			      Command & Control<BR>
			      Data Acquisition (see also Sensors)<BR>
			      Data Fusion<BR>
			      Data Input/Output Devices (Displays, Storage)<BR>
			      Data Modeling (see also Testing & Evaluation)<BR>
			      Data Processing<BR>
			      Development Environments<BR>
			      Display<BR>
			      Hardware-in-the-Loop Testing<BR>
			      Image Analysis<BR>
			      Image Capture (Stills/Motion)<BR>
			      Image Processing<BR>
			      Intelligence<BR>
			      Man-Machine Interaction<BR>
			      Models & Simulations (see also Testing & Evaluation)<BR>
			      Navigation & Guidance<BR>
			      Operating Systems<BR>
			      Optical<BR>
			      Perception/Vision<BR>
			      Positioning (Attitude Determination, Location X-Y-Z)<BR>
			      Prototyping<BR>
			      Relative Navigation (Interception, Docking, Formation Flying; see also Control & Monitoring; Planetary Navigation, Tracking, & Telemetry)<BR>
			      Robotics (see also Control & Monitoring; Sensors)<BR>
			      Simulation & Modeling<BR>
			      Software Tools (Analysis, Design)<BR>
			      Telemetry (see also Control & Monitoring)<BR>
			      Teleoperation<BR>
			      Vehicles (see also Autonomous Systems)<BR>
		      </DIV>
		    </TD>
		  </TR>
		 </TBODY>
		</TABLE>
		<HR noShade SIZE="3">
		<FONT size=-1>Form Generated on 02-28-13 11:49</FONT>
	</TD>
  </TR>
</TABLE>
</BODY>
</HTML>
